{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zafar Hussain (University of Helsinki, IVVES Project), Marcin Kowiel (F-Secure)\"\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    return round(SequenceMatcher(None, a, b).ratio(), 3)\n",
    "\n",
    "\n",
    "def draw_tree_multiple_command(tags, token_ids, tokens):\n",
    "    \"\"\"\n",
    "    We are calling this function when there is more than one COMM in the command.\n",
    "    nodes: tokens\n",
    "    labels: the labels we assigned to each token\n",
    "    We extracted the indices of COMM, and then while going over each index, if the index is in the extracted\n",
    "    indices, that index is marked as COMM, and the subsequent tokens are connected with this node.\n",
    "    We are labeling each edge with the label, e.g. COMM, SUBCOMM, FLAG etc.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    if len(tags) == 0:\n",
    "        return G\n",
    "    elif len(tags) == 1:\n",
    "        G.add_edge(token_ids[0], token_ids[0], label=tags[0], end_token=tokens[0])\n",
    "        return G\n",
    "\n",
    "    comm_indices = [i_tag for i_tag, tag in enumerate(tags) if tag == \"COMM\"]\n",
    "    j = 0\n",
    "    last_cmd = token_ids[0]\n",
    "\n",
    "    for i in range(0, len(tags)):\n",
    "        if i == comm_indices[j]:\n",
    "            G.add_edge(last_cmd, token_ids[comm_indices[j]], label=tags[comm_indices[j]], end_token=tokens[0])\n",
    "            last_cmd = token_ids[comm_indices[j]]\n",
    "            # if not the last command\n",
    "            if j + 1 < len(comm_indices):\n",
    "                j += 1\n",
    "\n",
    "        if tags[i] in (\"FLAG\", \"FLAG_WITH_FLAG_VALUE\", \"PARAM\", \"OPERATOR\", \"SUBCOMM\", \"COMM_SCRIPT\", \"SCRIPT\"):\n",
    "            G.add_edge(last_cmd, token_ids[i], label=tags[i], end_token=tokens[i])\n",
    "        elif tags[i] in (\"FLAG_VALUE\", \"FLAG_SEPARATOR\"):\n",
    "            G.add_edge(token_ids[i - 1], token_ids[i], label=tags[i], end_token=tokens[i])\n",
    "        elif tags[i] == \"OPERATOR\" and tags[i + 1] == \"OPERATOR_PARAM\":\n",
    "            G.add_edge(last_cmd, token_ids[i], label=tags[i], end_token=tokens[i])\n",
    "            G.add_edge(token_ids[i], token_ids[i + 1], label=tags[i + 1], end_token=tokens[i + 1])\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def program_name_win(command):\n",
    "    \"\"\"\n",
    "    Get program name for windows platform\n",
    "    return program name without extension, for example \"cmd\"  for \"C:\\\\windows\\\\sys\\\\cmd.exe\"\n",
    "    \"\"\"\n",
    "    return command.split(\"\\\\\")[-1].split(\".\")[0].replace('\"', \"\").replace(\"'\", \"\")\n",
    "\n",
    "\n",
    "def get_token(token_id):\n",
    "    return token_id[2]\n",
    "\n",
    "\n",
    "def get_tokens(token_ids):\n",
    "    return [token_id[2] for token_id in token_ids]\n",
    "\n",
    "\n",
    "def token_similarity(edge1, edge2):\n",
    "    return len(set(get_tokens(edge1)) & set(get_tokens(edge2))) / float(\n",
    "        len(set(get_tokens(edge1)) | set(get_tokens(edge2)))\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_combinations(U, gr_dt_1, gr_dt_2, similarity_combinations, commands_names, commands_distance_matrix):\n",
    "    \"\"\"\n",
    "    U: final graph\n",
    "    gr_dt_1: dictionary of tokens and labels of command one\n",
    "    gr_dt_2: dictionary of tokens and labels of command two\n",
    "    similarity_combinations: this is a reference table, where we have 54 combinations and their class\n",
    "    \"\"\"\n",
    "    U_edges = list(U.edges.data())\n",
    "    comm_value, subcomm_value, flag_value, param_value = 0, -1, -1, -1\n",
    "    result = []\n",
    "    unique_edges = []\n",
    "\n",
    "    comm_1_edges = gr_dt_1.keys()\n",
    "    comm_2_edges = gr_dt_2.keys()\n",
    "    for new_edge in U_edges:\n",
    "        edge_name = new_edge[2][\"label\"]\n",
    "        if edge_name not in unique_edges:\n",
    "            unique_edges.append(edge_name)\n",
    "    if unique_edges[0] != \"COMM\":\n",
    "        comm_index = unique_edges.index(\"COMM\")\n",
    "        unique_edges[0], unique_edges[comm_index] = \"COMM\", unique_edges[0]\n",
    "    if \"SUBCOMM\" in unique_edges and unique_edges[1] != \"SUBCOMM\":\n",
    "        subcomm_index = unique_edges.index(\"SUBCOMM\")\n",
    "        unique_edges[1], unique_edges[subcomm_index] = \"SUBCOMM\", unique_edges[1]\n",
    "\n",
    "    for edge_name in unique_edges:\n",
    "        if edge_name == \"COMM\":\n",
    "            # not similar\n",
    "            comm_value = 0\n",
    "\n",
    "            comm_1 = gr_dt_1[\"COMM\"]\n",
    "            comm_2 = gr_dt_2[\"COMM\"]\n",
    "            final_index_1 = max(index for index, item in enumerate(comm_1))\n",
    "            final_index_2 = max(index for index, item in enumerate(comm_2))\n",
    "            # if identical\n",
    "            if comm_1[final_index_1] == comm_2[final_index_2]:\n",
    "                logger.debug(\"COMM same\")\n",
    "                comm_value = 1\n",
    "            else:\n",
    "                base_cmd_sliced = program_name_win(get_token(comm_1[final_index_1]))\n",
    "                new_cmd_sliced = program_name_win(get_token(comm_2[final_index_2]))\n",
    "                # if program names the same but path different\n",
    "                if base_cmd_sliced == new_cmd_sliced:\n",
    "                    logger.debug(\"COMM same\")\n",
    "                    comm_value = 1\n",
    "                # if close names\n",
    "                elif (\n",
    "                    # file names similar\n",
    "                    similar(base_cmd_sliced, new_cmd_sliced) >= 0.95\n",
    "                    # and paths similar\n",
    "                    and similar(get_token(comm_1[final_index_1]), get_token(comm_2[final_index_2])) >= 0.95\n",
    "                ):\n",
    "                    comm_value = 1\n",
    "                # look for aliases\n",
    "                elif (\n",
    "                    commands_names is not None\n",
    "                    and base_cmd_sliced in commands_names\n",
    "                    and new_cmd_sliced in commands_names\n",
    "                ):\n",
    "                    command_row = np.where(commands_names == base_cmd_sliced)[0][0]\n",
    "                    command_col = np.where(commands_names == new_cmd_sliced)[0][0]\n",
    "                    sim_score_ref = round(commands_distance_matrix[command_row][command_col], 4)\n",
    "                    if sim_score_ref > 0.80:\n",
    "                        comm_value = 1\n",
    "\n",
    "            if comm_value == 1:\n",
    "                logger.debug(\"COMM same\")\n",
    "            else:\n",
    "                logger.debug(\"COMM not same\")\n",
    "        elif edge_name == \"SUBCOMM\":\n",
    "            print(\"SUBCOMM\", edge_name in comm_1_edges and edge_name in comm_2_edges)\n",
    "            subcomm_value = 0\n",
    "            # if missing\n",
    "            if edge_name not in comm_1_edges and edge_name not in comm_2_edges:\n",
    "                logger.debug(\"SUBCOMM missing\")\n",
    "                subcomm_value = -1\n",
    "\n",
    "            if edge_name in comm_1_edges and edge_name in comm_2_edges:\n",
    "                # if very close\n",
    "                res_subcomm = token_similarity(gr_dt_1[edge_name], gr_dt_2[edge_name])\n",
    "                print(res_subcomm, gr_dt_1[edge_name], gr_dt_2[edge_name])\n",
    "                if res_subcomm > 0.99:\n",
    "                    logger.debug(\"SUBCOMM same\")\n",
    "                    subcomm_value = 1\n",
    "\n",
    "            if subcomm_value == 0:\n",
    "                logger.debug(\"SUBCOMM not same\")\n",
    "\n",
    "        elif edge_name == \"FLAG\":\n",
    "            flag_value = 0\n",
    "            if edge_name not in comm_1_edges and edge_name not in comm_2_edges:\n",
    "                logger.debug(\"FLAG missing\")\n",
    "                flag_value = -1\n",
    "\n",
    "            if edge_name in comm_1_edges and edge_name in comm_2_edges:\n",
    "                res_flags = token_similarity(gr_dt_1[edge_name], gr_dt_2[edge_name])\n",
    "                if res_flags > 0.89:\n",
    "                    logger.debug(\"FLAG same\")\n",
    "                    flag_value = 1\n",
    "\n",
    "            if flag_value == 0:\n",
    "                logger.debug(\"FLAG not same\")\n",
    "\n",
    "        elif edge_name == \"PARAM\":\n",
    "            param_value = 0\n",
    "            print(\"PARAM\")\n",
    "            if edge_name not in comm_1_edges and edge_name not in comm_2_edges:\n",
    "                logger.debug(\"PARAM missing\")\n",
    "                param_value = -1\n",
    "\n",
    "            if edge_name in comm_1_edges and edge_name in comm_2_edges:\n",
    "                if comm_value == 1 or subcomm_value == 1:\n",
    "                    param_score = 0\n",
    "                    param_1 = get_tokens(gr_dt_1[\"PARAM\"])\n",
    "                    param_2 = get_tokens(gr_dt_2[\"PARAM\"])\n",
    "                    print(param_1, param_2)\n",
    "                    for z in range(max(len(param_1), len(param_2))):\n",
    "                        if z < len(param_1) and z < len(param_2):\n",
    "                            if param_1[z] == param_2[z] or similar(param_1[z], param_2[z]) >= 0.75:\n",
    "                                param_score += 1\n",
    "                            elif param_1[z].isnumeric() and param_2[z].isnumeric():\n",
    "                                param_score += 1\n",
    "\n",
    "                    if param_score / max(len(param_1), len(param_2)) > 0.65:\n",
    "                        logger.debug(\"PARAM same\")\n",
    "                        param_value = 1\n",
    "                else:\n",
    "                    res_param = token_similarity(gr_dt_1[edge_name], gr_dt_2[edge_name])\n",
    "                    print(gr_dt_1[edge_name], gr_dt_2[edge_name], res_param)\n",
    "                    if res_param > 0.65:\n",
    "                        logger.debug(\"PARAM same\")\n",
    "                        param_value = 1\n",
    "\n",
    "            if param_value == 0:\n",
    "                logger.debug(\"PARAM not same\")\n",
    "\n",
    "    result.append([comm_value, subcomm_value, flag_value, param_value])\n",
    "    result = tuple([item for sublist in result for item in sublist])\n",
    "    key_names = [\"COMM\", \"SUBCOMM\", \"FLAG\", \"PARAM\"]\n",
    "    logger.info([(r, name) for r, name in zip(result, key_names)])\n",
    "    print([(r, name) for r, name in zip(result, key_names)])\n",
    "    class_output = similarity_combinations[result]\n",
    "    return class_output\n",
    "\n",
    "\n",
    "def create_dict_nodes_labels(tags, tokens):\n",
    "    \"\"\"\n",
    "    Given nodes and labels, we create a dictionary where labels (COMM, SUBCOMM, FLAG etc.) are\n",
    "    keys and tokens are the values of these keys.\n",
    "    \"\"\"\n",
    "    grouped_data = dict()\n",
    "    for tag, token in zip(tags, tokens):\n",
    "        if tag not in grouped_data:\n",
    "            grouped_data[tag] = []\n",
    "        grouped_data[tag].append(token)\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "def token_with_tags(tags, tokens):\n",
    "    return [(tag, i_tag, token) for i_tag, tag, token in zip(range(len(tags)), tags, tokens)]\n",
    "\n",
    "\n",
    "def normalize_digits(tokens):\n",
    "    return [re.sub(\"\\d\", \"0\", token) for token in tokens]\n",
    "\n",
    "\n",
    "def normalize_uuid(tokens):\n",
    "    return [\n",
    "        re.sub(\n",
    "            \"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89abAB][0-9a-fA-F]{3}-[0-9a-fA-F]{12}\",\n",
    "            \"<<UUID>>\",\n",
    "            token,\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    "\n",
    "\n",
    "def compare_graphs(tags1, tokens1, tags2, tokens2, similarity_combinations, commands_names, commands_distance_matrix):\n",
    "    \"\"\"\n",
    "    Compare two command lines. Works only with windows platform.\n",
    "\n",
    "    tags1 and tokens1 represent the base_command (command one)\n",
    "    tags2 and tokens2 represent the new command which is being compared with the base command\n",
    "    similarity_combinations: this is a reference table, where we have 54 combinations and their class\n",
    "    For now we have faced one instance where COMM and FLAG had the same values, such as pip. So\n",
    "    we are marking COMM 'pip' as 'PIP' and FLAG 'pip' remains 'pip'.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_wth_prefix1 = token_with_tags(tags1, normalize_uuid(normalize_digits(tokens1)))\n",
    "    tokens_wth_prefix2 = token_with_tags(tags2, normalize_uuid(normalize_digits(tokens2)))\n",
    "\n",
    "    if \"COMM\" not in set(tags1) or \"COMM\" not in set(tags2):\n",
    "        raise Exception(\"'COMM' tag not detected\")\n",
    "\n",
    "    g1 = draw_tree_multiple_command(tags1, tokens_wth_prefix1, tokens1)\n",
    "    g2 = draw_tree_multiple_command(tags2, tokens_wth_prefix2, tokens2)\n",
    "\n",
    "    # Edges and nodes of both the generated trees are extracted here\n",
    "    g1_ed = list(g1.edges.data())\n",
    "    g2_ed = list(g2.edges.data())\n",
    "    g2_nd = list(g2.nodes)\n",
    "    g1_nd = list(g1.nodes)\n",
    "\n",
    "    # A new graph is created by uniting edges and nodes of both the trees.\n",
    "    combined_graph = nx.DiGraph()\n",
    "    combined_graph.add_edges_from(g1_ed + g2_ed)\n",
    "    combined_graph.add_nodes_from(g1_nd + g2_nd)\n",
    "\n",
    "    # Calling the function with nodes and labels to create dictionaries\n",
    "    gr_dt_1 = create_dict_nodes_labels(tags1, tokens_wth_prefix1)\n",
    "    gr_dt_2 = create_dict_nodes_labels(tags2, tokens_wth_prefix2)\n",
    "\n",
    "    # Classifying the given two commands.\n",
    "    class_output = verify_combinations(\n",
    "        combined_graph, gr_dt_1, gr_dt_2, similarity_combinations, commands_names, commands_distance_matrix\n",
    "    )\n",
    "\n",
    "    return \" \".join(tokens1), \" \".join(tokens2), class_output\n",
    "\n",
    "\n",
    "def load_similarity_config(similarity_definition_path=os.path.join(os.getcwd(), \"data\", \"similarity.csv\")):\n",
    "    with open(similarity_definition_path) as csvfile:\n",
    "        similarity_combinations = csv.reader(csvfile, delimiter=\",\", quotechar='\"')\n",
    "        similarity_combinations_dict = {}\n",
    "        for row in list(similarity_combinations)[1:]:\n",
    "            is_similar = True if row[4] == \"Similar\" else False\n",
    "            similarity_combinations_dict[(int(row[0]), int(row[1]), int(row[2]), int(row[3]))] = is_similar\n",
    "        return similarity_combinations_dict\n",
    "\n",
    "\n",
    "def load_similarity_matrix(win_cmd_similarity_matrix=os.path.join(os.getcwd(), \"data\", \"windows_desc_sim_df.csv\")):\n",
    "    with open(win_cmd_similarity_matrix) as csvfile:\n",
    "        wind_comd_sim_ref = csv.reader(csvfile, delimiter=\",\", quotechar='\"')\n",
    "        wind_comd_sim_ref = np.array(list(wind_comd_sim_ref))\n",
    "        wind_comd_sim_ref_columns = wind_comd_sim_ref[0][1:]\n",
    "        wind_comd_sim_ref = wind_comd_sim_ref[1:, 1:].astype(float)\n",
    "        return wind_comd_sim_ref_columns, wind_comd_sim_ref\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
